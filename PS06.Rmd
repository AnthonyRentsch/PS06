---
title: "STAT/MATH 495: Problem Set 06"
author: "Anthony Rentsch"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
library(gridExtra)
```



# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```


# Computation

```{r, echo = FALSE}
preds.2 <- data.frame()
preds.99 <- data.frame()
```

Loop through 10,000 random samples of points, train two models (one with $df = 2$ and one with $df = 99$), predict the value of f(x0), and record a value of f(x) + $\epsilon$. This creates two data frames with a row for every iteration, a predicted value, and a randomly generated "observed" value for f(x0).

```{r}
set.seed(176)

i <- 1
while(i <= n_sample){
  data <- generate_sample(f = f, n = n, sigma = sigma)
  
  model.df2 <- smooth.spline(data$x, data$y, df = 2)
  pred.2 <- predict(model.df2, test_set)
  preds.2[i,1] <- pred.2$y
  preds.2[i,2] <- (f(x0) + rnorm(n = 1, mean = 0, sd = sigma))
    
  model.df99 <- smooth.spline(data$x, data$y, df = 99)
  pred.99 <- predict(model.df99, test_set)
  preds.99[i,1] <- pred.99$y
  preds.99[i,2] <- (f(x0) + rnorm(n = 1, mean = 0, sd = sigma))
  
  i = i +1
}

names(preds.2) <- c("estimate", "observed")
names(preds.99) <- c("estimate", "observed")
```

Create functions to calculate MSE, bias, and variance.

```{r}
truth = f(x0)

mse <- function(preds) mean((preds$estimate - preds$obs) ^ 2)

bias_sq <- function(preds)  (mean(preds$estimate) - truth)^2
  
variance <- function(preds) mean((preds$estimate - mean(preds$estimate))^2)
```




# Tables


For the linear regression, i.e. `smooth.splines(x, y, df = 2)`:

```{r}
data.frame(
  "MSE" = mse(preds.2),
  "bias_squared" = bias_sq(preds.2),
  "variance" = variance(preds.2),
  "irreducible" = sigma^2,
  "sum" = bias_sq(preds.2) + variance(preds.2) + sigma^2
) %>% 
  knitr::kable(digits = 4)
```

For the `smooth.splines(x, y, df = 99)` model:

```{r}
data.frame(
  "MSE" = mse(preds.99),
  "bias_squared" = bias_sq(preds.99),
  "variance" = variance(preds.99),
  "irreducible" = sigma^2,
  "sum" = bias_sq(preds.99) + variance(preds.99) + sigma^2
) %>% 
  knitr::kable(digits = 4)
```




# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
1. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all* $x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
1. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. One possible sanity check would be to overlay each instance of each model, i.e., every `smooth.splines(x, y, df = 2)` and `smooth.splines(x, y, df = 99)`, over a scatterplot of the data and examine how much space exists between each instance of the model and how far off the model deviates from the data points. If what I see does not line up with what was presented in Lec 2.7, my work likely contains errors.
1. I would follow the outline I created under the $Computation$ section, except that I would calculate the MSE not just for the point f(x0), but for all x in my sample of size $n$.
1. While both models have their benefits, I would choose the the linear regression model, i.e., `smooth.splines(x, y, df = 2)`, for prediction. Even though the `smooth.splines(x, y, df = 99)` model predicts the correct value for $f(0.95)$ on average, the variation between resamples is so great that I do not feel confident that any particular model would produce an accurate estimate of the point of interest. On the other hand, a model with low variance will give a farily consistent prediction no matter what resample the model is trained on. Over time it may be possible to understand how the model's estimates become biased and to correct for this, something that would be more challenging to do for variance. Thus, I feel more confident that a slightly biased model would produce a prediction that could be adjusted over time. With this being said, selecting a model with a value for $df$ in the middle of these two is a better option.
